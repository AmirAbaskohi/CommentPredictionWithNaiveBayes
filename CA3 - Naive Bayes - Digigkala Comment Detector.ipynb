{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA3 - Naive Bayes\n",
    "\n",
    "### Amirhossein Abaskohi                              SID: 810197539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intoduction:\n",
    "\n",
    "In this project we are going to examine Naive Bayes Classifier on comment classifying on sample data of Digikala.\n",
    "\n",
    "Here we will use Preprocess data to have better guessing and also we will use Additive Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "from csv import reader\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Preprocess data\n",
    "\n",
    "In this part, we use hazm and some other codes to find main words and usable words for prediction.\n",
    "First we normalize data, that means we normalizing using spaces and half-spaces.\n",
    "Then we break string to its words. And Also we remove some stop words and unusuable signs and Also remove words with numbers and Englis letters.\n",
    "After that clean the words(clearing some signs) and put cleaned words into new list that can be useful.\n",
    "\n",
    "#### Question 1:\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "\n",
    "However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
    "\n",
    "Maybe this is in an information retrieval setting and you want to boost your algorithm’s recall. Or perhaps you are trying to analyze word usage in a corpus and wish to condense related words so that you don’t have as much variability. Either way, this technique of text normalization may be useful to you.\n",
    "\n",
    "I used lemmatization. In considerig the goal of a sentence, we just need meaning of words here. So for simplification here we remove words that are in same root, for example verbs. So here we can use them easily to identify the goal of sentence, we do not need to know that word is plural or not or which tense the verb is in. This can reduce number of words because we check roots and we delete words that are same. Also as mentioned it reduces variety of same root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARP_SIGN = '#'\n",
    "UNUSUABLE_SIGNS = {\n",
    "    '@', '#', '$', '%', '^', '&', '*', ')',\n",
    "    '(', ']', '[', '>', '<', '=', '+', '-',\n",
    "    '_', '|', '/', '.', ':', ';', '?', '!', '`', '~', '،', ' '\n",
    "}\n",
    "BACKSLASH = \"\\\\\"\n",
    "ONE_COUTE = \"\\'\"\n",
    "DOUBLE_COUTE = \"\\\"\"\n",
    "FUTURE_TENSE_SIGNS = [\"خواهم\", \"خواهی\", \"خواهد\", \"خواهیم\", \"خواهید\", \"خواهند\"]\n",
    "ENGLISH_LETTERS_AND_DIGITS = {\n",
    "    '1','2','3','4','5','6','7','8','9','0',\n",
    "    'a','b','c','d','e','f','g','h','i','j','k',\n",
    "    'l','m','n','o','p','q','r','s','t','u','v','w',\n",
    "    'x','y','z','A','B','C','D','E','F','G','H','I','J',\n",
    "    'K','L','M','N','O','P','Q','R','S','T','U','V','W',\n",
    "    'X','Y','Z','۱','۲','۳','۴','۵','۶','۷','۸','۹','۰'\n",
    "}\n",
    "UNUSABLE_WORDS = {\n",
    "    \"که\"\n",
    "    , \"از\"\n",
    "    , \"بر\"\n",
    "    , \"به\"\n",
    "    , \"با\"\n",
    "    , \"در\"\n",
    "    , \"روی\"\n",
    "    , \"اما\"\n",
    "    , \"فقط\"\n",
    "    , \"و\"\n",
    "    , \"هم\"\n",
    "    , \"آن\"\n",
    "    , \"این\"\n",
    "    , \"چیز\"\n",
    "    , \"زیر\"\n",
    "    , \"رو\"\n",
    "    , \"ها\"\n",
    "    , \"یا\"\n",
    "    , \"نه\"\n",
    "    , \"ولی\"\n",
    "    , \"تا\"\n",
    "    , \"ولو\"\n",
    "    , \"اگر\"\n",
    "    , \"اگه\"\n",
    "    , \"درصد\"\n",
    "    , \"بالا\"\n",
    "    , \"پایین\"\n",
    "    , \"چپ\"\n",
    "    , \"راست\"\n",
    "    , \"یه\"\n",
    "    , \"یک\"\n",
    "    , \"من\"\n",
    "    , \"تو\"\n",
    "    , \"او\"\n",
    "    , \"ما\"\n",
    "    , \"شما\"\n",
    "    , \"هر\"\n",
    "    , \"چون\"\n",
    "    , \"برای\"\n",
    "    , \"اینکه\"\n",
    "    , \"هست\"\n",
    "    , \"است\"\n",
    "    , \"نیست\"\n",
    "    , \"واقعا\"\n",
    "    , \"کرد\"\n",
    "    , \"شد\"\n",
    "    , \"شده\"\n",
    "    , \"شدم\"\n",
    "    , \"دو\"\n",
    "    , \"سه\"\n",
    "    , \"چهار\"\n",
    "    , \"چاهار\"\n",
    "    , \"پنج\"\n",
    "    , \"شش\"\n",
    "    , \"هفت\"\n",
    "    , \"هشت\"\n",
    "    , \"نه\"\n",
    "    , \"بار\"\n",
    "    , \"هزار\"\n",
    "    , \"میلیون\"\n",
    "    , \"تومن\"\n",
    "    , \"دلار\"\n",
    "    , \"میشه\"\n",
    "    , \"داشته\"\n",
    "    , \"باشه\"\n",
    "}\n",
    "lemmatizer = Lemmatizer()\n",
    "pNumberOfWordsInRecom = 0\n",
    "pNumberOfWordsInNotRecom = 0\n",
    "NumberOfWordsInRecom = 0\n",
    "NumberOfWordsInNotRecom = 0\n",
    "numberOfRecommends = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(string):\n",
    "    normalizer = Normalizer()\n",
    "    return normalizer.normalize(string)\n",
    "\n",
    "def changeIfIsFutureTense(word):\n",
    "    if len(word) < 6:\n",
    "        return word\n",
    "    firstFiveLetters = word[:5]\n",
    "    if firstFiveLetters in FUTURE_TENSE_SIGNS:\n",
    "        word = word[5:]\n",
    "        result = \"\"\n",
    "        for char in word:\n",
    "            if char == \"\\u200c\":\n",
    "                continue\n",
    "            else:\n",
    "                result += char\n",
    "        return result\n",
    "    return word\n",
    "\n",
    "def simplifyString(words):\n",
    "    for i in range(len(words)):\n",
    "        if(words[i] == \"برند\"):\n",
    "            continue\n",
    "        changedWord = changeIfIsFutureTense(words[i])\n",
    "        lemmatizedWord = lemmatizer.lemmatize(changedWord)\n",
    "        splittedBySharp = lemmatizedWord.split(SHARP_SIGN)\n",
    "        words[i] = splittedBySharp[0]\n",
    "        \n",
    "def cleanWord(word):\n",
    "    result = \"\"\n",
    "    for char in word:\n",
    "        if char not in UNUSUABLE_SIGNS and char != BACKSLASH and char != ONE_COUTE and char != DOUBLE_COUTE:\n",
    "            result += char\n",
    "    return result\n",
    "\n",
    "def isNumberOrEnglishLetter(word):\n",
    "    for char in ENGLISH_LETTERS_AND_DIGITS:\n",
    "        if char in word:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def removeUnusablesAndDuplicates(words):\n",
    "    outWords = list()\n",
    "    for word in words:\n",
    "        cleanedWord = cleanWord(word)\n",
    "        if cleanedWord == \"\" or cleanedWord in UNUSABLE_WORDS or isNumberOrEnglishLetter(cleanedWord):\n",
    "            continue\n",
    "        outWords.append(cleanedWord)\n",
    "    return outWords\n",
    "\n",
    "def preProcessData(comment):\n",
    "    normalizedComment = normalizeString(comment)\n",
    "    words = word_tokenize(normalizedComment)\n",
    "    simplifyString(words)\n",
    "    preProcessedData = removeUnusablesAndDuplicates(words)\n",
    "    return preProcessedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase2: Soving and Phase3: Assestment\n",
    "\n",
    "Here we have some train data. First we read this file using CSVReader of csv library. Then for every row in train file we do the following:\n",
    "\n",
    "* Preprocess the comment and title and concat them\n",
    "* In Preprocess words list, we check that it is in which class, then add to its key in dictionary\n",
    "\n",
    "So After this we have number of repetation of words in each class. We will use this to calculate the probabilities.\n",
    "\n",
    "#### Question 2:\n",
    "\n",
    "Posterior here is out final wanted result. It is P(recommended | w). Here w is showing words in the title and comment.\n",
    "\n",
    "Prior here is probability of being recommended without any evidence.\n",
    "\n",
    "Evidences here are the words.\n",
    "\n",
    "Likehood is showing probability of being a word in recommended class.\n",
    "\n",
    "Prior is calculated easily. It is just (number_of_recommended_trains/number_of_all_trains).\n",
    "\n",
    "For calculating likehood, we sum out all number of words in each category. Also we sum out number of repetation of each word in each catogory(talked in last markdown part). So the probability of one word in some category for example \n",
    "P(\"دی جی کالا\" | recommended) is ratio of repetation of \"دی جی کالا\" to all words in recommended.\n",
    "\n",
    "Posterior probability is calculated using top things. It is multiplication of P(recommended) * P(w1 | recommended) * P(w2 | recommended) ... ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comment_train.csv', 'r', encoding=\"utf8\") as readObj:\n",
    "    csvReader = reader(readObj)\n",
    "    listOfRows = list(csvReader)\n",
    "    del listOfRows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessedClassify(trains):\n",
    "    global pNumberOfWordsInRecom\n",
    "    global pNumberOfWordsInNotRecom\n",
    "    recommended = dict()\n",
    "    notRecommended = dict()\n",
    "    for train in trains:\n",
    "        words = preProcessData(train[0]) +  preProcessData(train[1])\n",
    "        if train[2] == \"recommended\":\n",
    "            pNumberOfWordsInRecom += len(words)\n",
    "        else:\n",
    "            pNumberOfWordsInNotRecom += len(words)\n",
    "        for word in words:\n",
    "            if word not in recommended:\n",
    "                recommended[word] = 0\n",
    "            if word not in notRecommended:\n",
    "                notRecommended[word] = 0\n",
    "            if train[2] == \"recommended\":\n",
    "                recommended[word] += 1\n",
    "            else:\n",
    "                notRecommended[word] += 1\n",
    "    return recommended, notRecommended\n",
    "\n",
    "def unPrePrecoessedClassify(trains):\n",
    "    global NumberOfWordsInRecom\n",
    "    global NumberOfWordsInNotRecom\n",
    "    recommended = dict()\n",
    "    notRecommended = dict()\n",
    "    for train in trains:\n",
    "        words = train[0].split() +  train[1].split()\n",
    "        if train[2] == \"recommended\":\n",
    "            NumberOfWordsInRecom += len(words)\n",
    "        else:\n",
    "            NumberOfWordsInNotRecom += len(words)\n",
    "        for word in words:\n",
    "            if word not in recommended:\n",
    "                recommended[word] = 0\n",
    "            if word not in notRecommended:\n",
    "                notRecommended[word] = 0\n",
    "            if train[2] == \"recommended\":\n",
    "                recommended[word] += 1\n",
    "            else:\n",
    "                notRecommended[word] += 1\n",
    "    return recommended, notRecommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr, pnr = preProcessedClassify(listOfRows)\n",
    "r, nr = unPrePrecoessedClassify(listOfRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessedIsCommentRecommended(comment, title):\n",
    "    words = preProcessData(comment) + preProcessData(title)\n",
    "    totalRecom = math.log10(numberOfRecommends/len(listOfRows))\n",
    "    totalNotRecom = math.log10((len(listOfRows) - numberOfRecommends)/len(listOfRows))\n",
    "    for word in words:\n",
    "        if word not in r:\n",
    "            continue\n",
    "        if pr[word] == 0:\n",
    "            return False\n",
    "        if pnr[word] == 0:\n",
    "            return True\n",
    "        totalRecom += math.log10(pr[word]/pNumberOfWordsInRecom)\n",
    "        totalNotRecom += math.log10(pnr[word]/pNumberOfWordsInNotRecom)\n",
    "    if totalRecom >= totalNotRecom:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isCommentRecommended(comment, title):\n",
    "    words = preProcessData(comment) + preProcessData(title)\n",
    "    totalRecom = math.log10(numberOfRecommends/len(listOfRows))\n",
    "    totalNotRecom = math.log10((len(listOfRows) - numberOfRecommends)/len(listOfRows))\n",
    "    for word in words:\n",
    "        if word not in r:\n",
    "            continue\n",
    "        if r[word] == 0:\n",
    "            return False\n",
    "        if nr[word] == 0:\n",
    "            return True\n",
    "        totalRecom += math.log10(r[word]/NumberOfWordsInNotRecom)\n",
    "        totalNotRecom += math.log10(nr[word]/NumberOfWordsInNotRecom)\n",
    "    if totalRecom >= totalNotRecom:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comment_test.csv', 'r', encoding=\"utf8\") as readObj:\n",
    "    csvReader = reader(readObj)\n",
    "    tests = list(csvReader)\n",
    "    del tests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(tests):\n",
    "    passedTests = 0\n",
    "    numberOfRecommended = 0\n",
    "    numberOfCorrectRecommended = 0\n",
    "    numberOfRecmGuess = 0\n",
    "    for test in tests:\n",
    "        guess = isCommentRecommended(test[1], test[0])\n",
    "        if test[2] == \"recommended\":\n",
    "            numberOfRecommended += 1\n",
    "        if guess:\n",
    "            numberOfRecmGuess += 1\n",
    "        if  guess and test[2] == \"recommended\":\n",
    "            passedTests += 1\n",
    "            numberOfCorrectRecommended += 1\n",
    "        elif not guess and test[2] != \"recommended\":\n",
    "            passedTests += 1\n",
    "    accuracy = (passedTests / len(tests))\n",
    "    presision = (numberOfCorrectRecommended/numberOfRecmGuess)\n",
    "    recall = (numberOfCorrectRecommended/numberOfRecommended)\n",
    "    F1 = 2 * ((presision*recall)/(presision+recall))\n",
    "    return accuracy, presision, recall, F1\n",
    "\n",
    "def preProcessedTest(test):\n",
    "    passedTests = 0\n",
    "    numberOfRecommended = 0\n",
    "    numberOfCorrectRecommended = 0\n",
    "    numberOfRecmGuess = 0\n",
    "    for test in tests:\n",
    "        guess = preProcessedIsCommentRecommended(test[1], test[0])\n",
    "        if test[2] == \"recommended\":\n",
    "            numberOfRecommended += 1\n",
    "        if guess:\n",
    "            numberOfRecmGuess += 1\n",
    "        if  guess and test[2] == \"recommended\":\n",
    "            passedTests += 1\n",
    "            numberOfCorrectRecommended += 1\n",
    "        elif not guess and test[2] != \"recommended\":\n",
    "            passedTests += 1\n",
    "    accuracy = (passedTests / len(tests))\n",
    "    presision = (numberOfCorrectRecommended/numberOfRecmGuess)\n",
    "    recall = (numberOfCorrectRecommended/numberOfRecommended)\n",
    "    F1 = 2 * ((presision*recall)/(presision+recall))\n",
    "    return accuracy, presision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pAcc, pPre, pRec, pF1 = preProcessedTest(tests)\n",
    "Acc, Pre, Rec, F1 = test(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in preprocess test: 0.9025\n",
      "Accuracy in unpreprocess test: 0.86\n",
      "-------------------------------------------------------\n",
      "Presision in preprocess test: 0.8946078431372549\n",
      "Presision in unpreprocess test: 0.8380281690140845\n",
      "-------------------------------------------------------\n",
      "Recall in preprocess test: 0.9125\n",
      "Recall in unpreprocess test: 0.8925\n",
      "-------------------------------------------------------\n",
      "F1 in preprocess test: 0.9034653465346535\n",
      "F1 in unpreprocess test: 0.864406779661017\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy in preprocess test: \" + str(pAcc))\n",
    "print(\"Accuracy in unpreprocess test: \" + str(Acc))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Presision in preprocess test: \" + str(pPre))\n",
    "print(\"Presision in unpreprocess test: \" + str(Pre))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Recall in preprocess test: \" + str(pRec))\n",
    "print(\"Recall in unpreprocess test: \" + str(Rec))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"F1 in preprocess test: \" + str(pF1))\n",
    "print(\"F1 in unpreprocess test: \" + str(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "left = [1,2,3,4,5,6,7,8]\n",
    "height = [pAcc, Acc, pPre, Pre, pRec, Rec, pF1, F1]\n",
    "tickLabel = ['pAcc', 'Acc', 'pPre', 'Pre', 'pRec', 'Rec', 'pF1', 'F1']\n",
    "plt.bar(left, height, tick_label = tickLabel, \n",
    "        width = 0.6, color = ['green', 'blue']) \n",
    "plt.xlabel('Values') \n",
    "plt.ylabel('Assestment') \n",
    "plt.title('Chart of Assestments without Additive Smoothing') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using additive smoothing:\n",
    "\n",
    "Additive smoothing is a way to estimate probability. In this we we would not have zero probability and can be better estimation for probabilities to predict class of one comment.\n",
    "\n",
    "#### Question 3:\n",
    "\n",
    "When one word is just used in one category, so the probability in other category will be zero. As we mentioned in Question2, posterior is multplication. So when probability of one word in one special category is 0, that makes all zero and so other caregory absolutely will be the answer and we know it is not true.\n",
    "\n",
    "#### Question 4:\n",
    "\n",
    "In statistics, additive smoothing, also called Laplace smoothing (not to be confused with Laplacian smoothing as used in image processing), or Lidstone smoothing, is a technique used to smooth categorical data. The formula is:\n",
    "\n",
    "![alt text](1.png)\n",
    "\n",
    "Here alpha is smoothing paramter that we set it as 1. d is number of different type of values.\n",
    "\n",
    "But how this help us? As we mentioned when a probability is zero that make all of chance of that group zero, but we know it is not true and other words are so effective. So when we use additive smoothing because of (+alpha) at top of ration, we would not have zero probability. So all words will be considered in calculation so we can have better predictions.\n",
    "\n",
    "Calculation of addtivce smoothing is done in calAdditiveProbability function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDifferentUsedWordsRecom = 0 \n",
    "pDifferentUsedWordsNotRecom = 0\n",
    "differentUsedWordsRecom = 0\n",
    "differentUsedWordsNotRecom = 0\n",
    "for x in r.values():\n",
    "    if x != 0:\n",
    "        differentUsedWordsRecom += 1\n",
    "for x in nr.values():\n",
    "    if x != 0:\n",
    "        differentUsedWordsNotRecom += 1\n",
    "for x in pr.values():\n",
    "    if x != 0:\n",
    "        pDifferentUsedWordsRecom += 1\n",
    "for x in pnr.values():\n",
    "    if x != 0:\n",
    "        pDifferentUsedWordsNotRecom += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calAdditiveProbability(x, N, alpha, d):\n",
    "    return (x+alpha)/(N + alpha * d)\n",
    "\n",
    "def preProcessedIsCommentRecommendedAdditiveSmoothing(comment, title):\n",
    "    words = comment.split() + title.split()\n",
    "    totalRecom = math.log10(calAdditiveProbability(numberOfRecommends, len(listOfRows), 1, 2))\n",
    "    totalNotRecom = math.log10(calAdditiveProbability(len(listOfRows)-numberOfRecommends, len(listOfRows), 1, 2))\n",
    "    for word in words:\n",
    "        if word not in r:\n",
    "            continue\n",
    "        totalRecom += math.log10(calAdditiveProbability(r[word], NumberOfWordsInRecom, 1, differentUsedWordsRecom))\n",
    "        totalNotRecom += math.log10(calAdditiveProbability(nr[word], NumberOfWordsInNotRecom, 1, differentUsedWordsNotRecom))\n",
    "    if totalRecom >= totalNotRecom:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isCommentRecommendedAdditiveSmoothing(comment, title):\n",
    "    words = preProcessData(comment) + preProcessData(title)\n",
    "    totalRecom = math.log10(calAdditiveProbability(numberOfRecommends, len(listOfRows), 1, 2))\n",
    "    totalNotRecom = math.log10(calAdditiveProbability(len(listOfRows)-numberOfRecommends, len(listOfRows), 1, 2))\n",
    "    for word in words:\n",
    "        if word not in pr:\n",
    "            continue\n",
    "        totalRecom += math.log10(calAdditiveProbability(pr[word], pNumberOfWordsInRecom, 1, pDifferentUsedWordsRecom))\n",
    "        totalNotRecom += math.log10(calAdditiveProbability(pnr[word], pNumberOfWordsInNotRecom, 1, pDifferentUsedWordsNotRecom))\n",
    "    if totalRecom >= totalNotRecom:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def testAdditive(tests):\n",
    "    passedTests = 0\n",
    "    numberOfRecommended = 0\n",
    "    numberOfCorrectRecommended = 0\n",
    "    numberOfRecmGuess = 0\n",
    "    for test in tests:\n",
    "        guess = isCommentRecommendedAdditiveSmoothing(test[1], test[0])\n",
    "        if guess:\n",
    "            numberOfRecmGuess += 1\n",
    "        if test[2] == \"recommended\":\n",
    "            numberOfRecommended += 1\n",
    "        if guess and test[2] == \"recommended\":\n",
    "            passedTests += 1\n",
    "            numberOfCorrectRecommended += 1\n",
    "        elif not guess and test[2] != \"recommended\":\n",
    "            passedTests += 1\n",
    "    accuracy = (passedTests / len(tests))\n",
    "    presision = (numberOfCorrectRecommended/numberOfRecmGuess)\n",
    "    recall = (numberOfCorrectRecommended/numberOfRecommended)\n",
    "    F1 = 2 * ((presision*recall)/(presision+recall))\n",
    "    return accuracy, presision, recall, F1\n",
    "\n",
    "def preTestAdditive(tests):\n",
    "    passedTests = 0\n",
    "    numberOfRecommended = 0\n",
    "    numberOfCorrectRecommended = 0\n",
    "    numberOfRecmGuess = 0\n",
    "    for test in tests:\n",
    "        guess = preProcessedIsCommentRecommendedAdditiveSmoothing(test[1], test[0])\n",
    "        if guess:\n",
    "            numberOfRecmGuess += 1\n",
    "        if test[2] == \"recommended\":\n",
    "            numberOfRecommended += 1\n",
    "        if guess and test[2] == \"recommended\":\n",
    "            passedTests += 1\n",
    "            numberOfCorrectRecommended += 1\n",
    "        elif not guess and test[2] != \"recommended\":\n",
    "            passedTests += 1\n",
    "    accuracy = (passedTests / len(tests))\n",
    "    presision = (numberOfCorrectRecommended/numberOfRecmGuess)\n",
    "    recall = (numberOfCorrectRecommended/numberOfRecommended)\n",
    "    F1 = 2 * ((presision*recall)/(presision+recall))\n",
    "    return accuracy, presision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "apAcc, apPre, apRec, apF1 = preTestAdditive(tests)\n",
    "aAcc, aPre, aRec, aF1 = testAdditive(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in preprocess test: 0.93875\n",
      "Accuracy in unpreprocess test: 0.92875\n",
      "-------------------------------------------------------\n",
      "Presision in preprocess test: 0.9354838709677419\n",
      "Presision in unpreprocess test: 0.9234567901234568\n",
      "-------------------------------------------------------\n",
      "Recall in preprocess test: 0.9425\n",
      "Recall in unpreprocess test: 0.935\n",
      "-------------------------------------------------------\n",
      "F1 in preprocess test: 0.9389788293897883\n",
      "F1 in unpreprocess test: 0.929192546583851\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy in preprocess test: \" + str(apAcc))\n",
    "print(\"Accuracy in unpreprocess test: \" + str(aAcc))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Presision in preprocess test: \" + str(apPre))\n",
    "print(\"Presision in unpreprocess test: \" + str(aPre))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Recall in preprocess test: \" + str(apRec))\n",
    "print(\"Recall in unpreprocess test: \" + str(aRec))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"F1 in preprocess test: \" + str(apF1))\n",
    "print(\"F1 in unpreprocess test: \" + str(aF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "left = [1,2,3,4,5,6,7,8]\n",
    "height = [apAcc, aAcc, apPre, aPre, apRec, aRec, apF1, aF1]\n",
    "tickLabel = ['apAcc', 'aAcc', 'apPre', 'aPre', 'apRec', 'aRec', 'apF1', 'aF1']\n",
    "plt.bar(left, height, tick_label = tickLabel, \n",
    "        width = 0.6, color = ['green', 'blue']) \n",
    "plt.xlabel('Values') \n",
    "plt.ylabel('Assestment') \n",
    "plt.title('Chart of Assestments with Additive Smoothing') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "\n",
    "Precision by its own, is not enough. Imagine a dataset that in which there are 800 tests and 700 are recommended and 100 are not. Now imagine we just found just 3 recommended al there were true. So out precision is 1 but we k that we have detected 697 truely. So it is not showing us information about false detections.\n",
    "\n",
    "Recall also is not enough bt its own. Imagine we have a dataset in which there are 800 tests and 700 are not recommended and 100 are recommended. Now imagine we detected all recommended. So the recall is 1 but it is not true we detected 700 tests wrong. It is because we do not consider false detections again. We detect some which were not recommended as recommended.\n",
    "\n",
    "#### Question 6:\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall taking both metrics into account in the following equation:\n",
    "![alt text](2.png)\n",
    "We use the harmonic mean instead of a simple average because it punishes extreme values.\n",
    "\n",
    " A classifier with a precision of 1.0 and a recall of 0.0 has a simple average of 0.5 but an F1 score of 0. The F1 score gives equal weight to both measures and is a specific example of the general Fβ metric where β can be adjusted to give more weight to either recall or precision. \n",
    " \n",
    " If we want to create a balanced classification model with the optimal balance of recall and precision, then we try to maximize the F1 score.\n",
    " \n",
    " #### Question 7:\n",
    " \n",
    " ![alt text](3.png)\n",
    " \n",
    " #### Question 8:\n",
    " \n",
    " As we see in the above table, when using preprocess, so we put same root words as one word. So this can make us to have better prediction. As  the  number  of  values  increases  (which  is  common  in  marketing  applications), preprocessing the categorical attributes becomes attractive in order to improve the performance  of  classifiers.  The  issue  of  grouping  methods  is  to  reduce  the  number  of  groups  of  values while maintaining the conditional class information.\n",
    "\n",
    "Also we can understand that additive smoothing makes our predictions better, because it consider all words and some word that are just in one special groups does not affect on our prediction.\n",
    "\n",
    "So using additive and preprocessing improve our prediction. Also the way of preprocessing is so important and a bad preprocessing can have bad influences.\n",
    "\n",
    "#### Question 9:\n",
    "\n",
    "*  تازه خریدم یه مدت کار بکنه مشخص میشه کیفیت قطعاتش - وری گود\n",
    "*  با این قیمت گزینه های بهتری هم میشه گرفت.روان مینویسه ولی زیاد مناسب نیست و رنگ پس میده یه وقتایی موقع نوشتن - زیاد مناسب نیست رنگ پس میده یه وقتایی موقع نوشتن\n",
    "*  کاور مقاوم و قشنگیه اما متأسفانه مدت زیادیه که ناموجوده...اگر موجود بشه قطعاً سه تا سفارش می‌دمرنگ طلاییش که فوق‌العاده زیباست و طراحی لبه‌ها به نحویه که دست رو اذیت نمی‌کنهلطفاً موجود بشه ممنون - mi 4w\n",
    "*  فندک قبلیم مدام فیوز میسوزوند و یک بار شارژر موبایل هم سوزوند ولی با این هیچ مشکلی بوجود نیومده تا الان. کیفیتش خیلی خوبه و لامپ هم داره - خیالم راحت شد\n",
    "*  ایراد دستگاه - ایراد دستگاه\n",
    "\n",
    "![alt text](4.gif)\n",
    "\n",
    "These are 5 comments and their titles that we had wrong detection. \n",
    "\n",
    "Learning a Naive Bayes classifier is just a matter of counting how many times each attribute co-occurs with each class. Naive Bayes is the most simple algorithm that you can apply to your data. As the name suggests, here this algorithm makes an assumption as all the variables in the dataset is “Naive” i.e not correlated to each other. This is what we did here: we used bag of words, although we know words are correlated to each other. For example in second problem that mentioned we have: \"بهتر\" but it is not showing positive comment here, althought the word itself is positive.\n",
    "\n",
    "Another problem that was here is that when we met some new word that we do not seen it before, we skip it, it is better to mke a dictionary table that shows positiveness of one word. So we can include those words to have better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "One limitation Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
    "\n",
    "But Naive Bayes is so simple to implement and it is fast. In implementation using Smoothing and Preprocesses can be so useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
